"""Samplers module.

This module contains classes used for raw dataset rebalancing or augmentation.

All samplers here should aim to be compatible with PyTorch's sampling interface
(torch.utils.data.sampler.Sampler) so that they can be instantiated at runtime
through a configuration file and used as the input of a data loader.
"""
import copy
import logging

import numpy as np
import torch
import torch.utils.data.sampler

logger = logging.getLogger(__name__)


class WeightedSubsetRandomSampler(torch.utils.data.sampler.Sampler):
    r"""Provides a rebalanced list of sample indices to use in a data loader.

    Given a list of sample indices and the corresponding list of class labels, this sampler
    will produce a new list of indices that rebalances the distribution of samples according
    to a specified strategy. It can also optionally scale the dataset's total sample count to
    avoid undersampling large classes as smaller ones get bigger.

    The currently implemented strategies are:

      * ``random``: will return a list of randomly picked samples based on the multinomial
        distribution of the initial class weights. This sampling is done with replacement,
        meaning that each index is picked independently of the already-picked ones.

      * ``uniform``: will rebalance the dataset by normalizing the sample count of all classes,
        oversampling and undersampling as required to distribute all samples equally. All
        removed or duplicated samples are selected randomly without replacement.

      * ``root``: will rebalance the dataset by normalizing class weight using an n-th degree
        root. More specifically, for a list of initial class weights :math:`W^0=\{w_1^0, w_2^0, ... w_n^0\}`,
        we compute the adjusted weight :math:`w_i` of each class via:

        .. math::
          w_i = \frac{\sqrt[\leftroot{-1}\uproot{3}n]{w_i^0}}{\sum_j\sqrt[\leftroot{-1}\uproot{3}n]{w_j^0}}

        Then, according to the new distribution of weights, all classes are oversampled and
        undersampled as required to reobtain the dataset's total sample count (which may be
        scaled). All removed or duplicated samples are selected randomly without replacement.

        Note that with the ``root`` strategy, if a very large root degree ``n`` is used, this
        strategy is equivalent to ``uniform``. The ``root`` strategy essentially provides a
        solution for extremely unbalanced datasets where uniform oversampling and undersampling
        would be too aggressive.

    By default, this interface will try to keep the dataset size constant and balance oversampling
    with undersampling. If undersampling is undesired, the user can increase the total dataset
    size via a scale factor. Finally, note that the rebalanced list of indices is generated by
    this interface every time the __iter__  function is called, meaning two consecutive lists
    might not contain the exact same indices.

    Attributes:
        nb_samples: total number of samples to rebalance (i.e. scaled size of original dataset)
        label_groups: map that splits all samples indices into groups based on labels
        pow: exponential power used to scale class weights in ``root`` strategy
        stype: name of the rebalancing strategy to use
        indices: copy of the original list of sample indices provided in the constructor
        weights: original class weights used in the ``random`` strategy
        label_weights: readjusted class weights used in the ``uniform`` and ``root`` strategies
        label_counts: number of samples in each class for the ``uniform`` and ``root`` strategies
    """

    def __init__(self, indices, labels, stype="uniform", scale=1.0):
        """Receives sample indices, labels, rebalancing strategy, and dataset scaling factor.

        This function will validate all input arguments, parse and categorize samples according to
        labels, initialize rebalancing parameters, and determine sample counts for each valid class.
        Note that an empty list of indices is an acceptable input; the resulting object will also
        create and empty list of samples when __iter__ is called.

        Args:
            indices: list of integers representing the indices of samples of interest in the dataset.
            labels: list of labels tied to the list of indices (must be the same length).
            stype: rebalancing strategy given as a string. Should be either "random", "uniform", or
                   "rootX", where the 'x' is the degree to use in the root computation (float).
            scale: scaling factor used to increase/decrease the final number of sample indices to
                   generate while rebalancing.
        """
        super().__init__(None)
        if not isinstance(indices, list) or not isinstance(labels, list):
            raise AssertionError("expected indices and labels to be provided as lists")
        if len(indices) != len(labels):
            raise AssertionError("mismatched indices/labels list sizes")
        if not isinstance(scale, float) or scale < 0:
            raise AssertionError("invalid scale parameter; should be greater than zero")
        self.nb_samples = int(round(len(indices) * scale))
        if self.nb_samples > 0:
            self.label_groups = {}
            if not isinstance(stype, str) or (stype not in ["uniform", "random"] and "root" not in stype):
                raise AssertionError("unexpected sampling type")
            if "root" in stype:
                self.pow = 1.0 / float(stype.split("root", 1)[1])  # will be the inverse power to use for rooting weights
                self.stype = "root"
            else:
                self.stype = stype
            self.indices = copy.deepcopy(indices)
            for idx, label in enumerate(labels):
                if label in self.label_groups:
                    self.label_groups[label].append(indices[idx])
                else:
                    self.label_groups[label] = [indices[idx]]
            if self.stype == "random":
                self.weights = [1.0 / len(self.label_groups[label]) for label in labels]
            else:
                if self.stype == "uniform":
                    self.label_weights = {label: 1.0 / len(self.label_groups) for label in self.label_groups}
                else:  # self.stype == "root"
                    self.label_weights = {label: (len(idxs) / len(labels)) ** self.pow for label, idxs in self.label_groups.items()}
                    tot_weight = sum([w for w in self.label_weights.values()])
                    self.label_weights = {label: weight / tot_weight for label, weight in self.label_weights.items()}
                self.label_counts = {}
                curr_nb_samples, max_sample_label = 0, None
                for label_idx, (label, indices) in enumerate(self.label_groups.items()):
                    self.label_counts[label] = int(self.nb_samples * self.label_weights[label])
                    curr_nb_samples += self.label_counts[label]
                    if max_sample_label is None or len(self.label_groups[label]) > len(self.label_groups[max_sample_label]):
                        max_sample_label = label
                if curr_nb_samples != self.nb_samples:
                    self.label_counts[max_sample_label] += self.nb_samples - curr_nb_samples

    def __iter__(self):
        """Returns the list of rebalanced sample indices to load.

        Note that the indices are repicked every time this function is called, meaning that samples
        eliminated due to undersampling (or duplicated due to oversampling) might not receive the same
        treatment twice.
        """
        if self.nb_samples == 0:
            return iter([])
        if self.stype == "random":
            return (self.indices[idx] for idx in torch.multinomial(self.weights, self.nb_samples, replacement=True))
        elif self.stype == "uniform" or self.stype == "root":
            indices = []
            for label, count in self.label_counts.items():
                max_samples = len(self.label_groups[label])
                while count > 0:
                    subidxs = torch.randperm(max_samples)
                    for subidx in range(min(count, max_samples)):
                        indices.append(self.label_groups[label][subidxs[subidx]])
                    count -= max_samples
            np.random.shuffle(indices)  # to make sure labels are still mixed up
            if len(indices) != self.nb_samples:
                raise AssertionError("messed up something internally...")
            return iter(indices)

    def __len__(self):
        """Returns the number of sample indices that will be generated by this interface.

        This number is the scaled size of the originally provided sample indices list.
        """
        return self.nb_samples
